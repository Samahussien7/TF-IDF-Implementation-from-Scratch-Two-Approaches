{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "!pip install stopwords\n",
        "!pip install requests beautifulsoup4 nltk\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "!python -m nltk.downloader wordnet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dK59xYwPNeqV",
        "outputId": "83ed277d-8d4d-47ba-f182-46baab3141e5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n",
            "Collecting stopwords\n",
            "  Downloading stopwords-1.0.1-py2.py3-none-any.whl (37 kB)\n",
            "Installing collected packages: stopwords\n",
            "Successfully installed stopwords-1.0.1\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.2.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/python3.10/runpy.py:126: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generate** **Document**"
      ],
      "metadata": {
        "id": "XJrHO__67Fnd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUVeSuenNDCy",
        "outputId": "cb2333e9-ba65-498a-c53c-7602ac4b6726"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Finance ---\n",
            "Navigating the complexities of modern finance.\n",
            "\n",
            "The first step is to understand the financial system. The financial system is a complex system that is complex because it is based on a set of rules and regulations. The rules and regulations are based on the rules and regulations of the financial system. The rules and regulations are based on the rules and regulations of the financial system. The rules and regulations are based on the rules and regulations of the financial system. The rules and regulations are based on the rules and regulations of the financial system. The rules and regulations are based on the rules and regulations of the financial system. The rules and regulations are based on the rules and regulations of the financial system. The rules and regulations are based on the rules and regulations of the financial system. The rules and regulations are based on the rules and regulations of the financial system. The rules and regulations are based on the rules and regulations of the financial system. The rules and regulations are based on the rules and regulations of the financial\n",
            "\n",
            "\n",
            "--- Education ---\n",
            "Revolutionizing education through technology.\n",
            "\n",
            "The first step in this process is to create a new system of education that is based on the principles of the American Revolution. The American Revolution was a revolution in the education of the children of the poor. It was a revolution in the education of the children of the rich. It was a revolution in the education of the children of the poor. It was a revolution in the education of the children of the rich. It was a revolution in the education of the children of the poor. It was a revolution in the education of the children of the poor. It was a revolution in the education of the children of the poor. It was a revolution in the education of the children of the poor. It was a revolution in the education of the children of the poor. It was a revolution in the education of the children of the poor. It was a revolution in the education of the children of the poor. It was a revolution in the education of the children of\n",
            "\n",
            "\n",
            "--- Environment ---\n",
            "Protecting the environment for a sustainable future.\n",
            "\n",
            "The Environmental Protection Agency (EPA) is the nation's largest agency, responsible for regulating the environment and protecting the environment for a sustainable future. The agency is responsible for protecting the environment for a sustainable future. The EPA is responsible for protecting the environment for a sustainable future. The EPA is responsible for protecting the environment for a sustainable future. The EPA is responsible for protecting the environment for a sustainable future.\n",
            "\n",
            "The EPA is responsible for protecting the environment for a sustainable future. The EPA is responsible for protecting the environment for a sustainable future. The EPA is responsible for protecting the environment for a sustainable future. The EPA is responsible for protecting the environment for a sustainable future. The EPA is responsible for protecting the environment for a sustainable future. The EPA is responsible for protecting the environment for a sustainable future. The EPA is responsible for protecting the environment for a sustainable future. The EPA is responsible for protecting the environment for a sustainable future. The EPA is\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "# Load pre-trained model and tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Function to generate text for a specific field\n",
        "def generate_document(prompt, max_length=200, temperature=0.9):\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "    output = model.generate(input_ids, max_length=max_length, temperature=temperature)\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Define fields and their prompts\n",
        "fields = {\n",
        "    \"finance\": \"Navigating the complexities of modern finance.\",\n",
        "    \"education\": \"Revolutionizing education through technology.\",\n",
        "    \"environment\": \"Protecting the environment for a sustainable future.\",\n",
        "}\n",
        "\n",
        "generated_documents = {}\n",
        "\n",
        "# Generate documents for different fields\n",
        "for field, prompt in fields.items():\n",
        "    generated_documents[field] = generate_document(prompt)\n",
        "\n",
        "# Post-processing to remove duplicate consecutive lines\n",
        "for field in generated_documents:\n",
        "    lines = generated_documents[field].split('\\n')\n",
        "    unique_lines = [lines[0]]\n",
        "    for line in lines[1:]:\n",
        "        if line != unique_lines[-1]:\n",
        "            unique_lines.append(line)\n",
        "    generated_documents[field] = '\\n'.join(unique_lines)\n",
        "\n",
        "# Print generated documents\n",
        "for field, document in generated_documents.items():\n",
        "    print(f\"--- {field.capitalize()} ---\")\n",
        "    print(document)\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Processing** **on** **data**"
      ],
      "metadata": {
        "id": "P5qtdV894-ri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction import text\n",
        "import pandas as pd\n",
        "\n",
        "# Step 1: Processing the generated documents\n",
        "processed_documents = {}\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "porter_stemmer = PorterStemmer()\n",
        "\n",
        "for field, document in generated_documents.items():\n",
        "    # Cleaning\n",
        "    cleaned_text = ''.join([char if char.isalnum() or char.isspace() else ' ' for char in document])\n",
        "\n",
        "    # Normalization\n",
        "    normalized_text = cleaned_text.lower()\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = normalized_text.split()\n",
        "\n",
        "    # Lemmatization and Stop words removal\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in text.ENGLISH_STOP_WORDS]\n",
        "\n",
        "    # Stemming\n",
        "    stemmed_tokens = [porter_stemmer.stem(token) for token in lemmatized_tokens]\n",
        "\n",
        "    # Filter out words that have no meaning in WordNet\n",
        "    meaningful_words = [word for word in stemmed_tokens if wordnet.synsets(word)]\n",
        "\n",
        "    # Filter out short words\n",
        "    meaningful_words = [word for word in meaningful_words if len(word) > 2]\n",
        "\n",
        "    processed_documents[field] = ' '.join(meaningful_words)\n",
        "\n",
        "    # Print unique words\n",
        "    unique_words = set(meaningful_words)\n",
        "    print(\"Unique words in document '{}': {}\".format(field, unique_words))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZPNi8HMvyYM",
        "outputId": "ecd31007-5f32-4da5-a7e1-d0a509a0e44f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique words in document 'finance': {'modern', 'base', 'set', 'step', 'rule', 'understand', 'complex'}\n",
            "Unique words in document 'education': {'base', 'revolution', 'new', 'poor', 'step', 'american', 'process', 'child', 'rich'}\n",
            "Unique words in document 'environment': {'environment', 'environ', 'nation', 'epa', 'protect', 'sustain', 'largest'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TF  IDF   From scratch**"
      ],
      "metadata": {
        "id": "VM5RcIfKEgQc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Get** **TF** **for** **each** **word** **for** **all** **documents** tf=(freq of word in doc)"
      ],
      "metadata": {
        "id": "jDLAHmmeOlMy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        "# Step 2: Calculate TF for each word in all documents\n",
        "corpus = list(processed_documents.values())\n",
        "vectorizer = CountVectorizer()\n",
        "term_counts = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Get the feature names (unique words)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Print TF for each word in all documents\n",
        "for i, word in enumerate(feature_names):\n",
        "    tf_values = term_counts[:, i].toarray().flatten()\n",
        "# Convert the matrix to DataFrame\n",
        "tf_df = pd.DataFrame(term_counts.toarray(), columns=feature_names, index=processed_documents.keys())\n",
        "\n",
        "# Print the TF table\n",
        "print(\"Term Frequency (TF) for each word in all documents:\")\n",
        "print(tf_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba380af4-5b5f-48c0-e8c5-e6f58da2fa11",
        "id": "XaGr_dYeKhBo"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Term Frequency (TF) for each word in all documents:\n",
            "             american  base  child  complex  environ  environment  epa  \\\n",
            "finance             0    11      0        3        0            0    0   \n",
            "education           2     1     12        0        0            0    0   \n",
            "environment         0     0      0        0       15            1   13   \n",
            "\n",
            "             largest  modern  nation  ...  poor  process  protect  revolution  \\\n",
            "finance            0       1       0  ...     0        0        0           0   \n",
            "education          0       0       0  ...     9        1        0           1   \n",
            "environment        1       0       1  ...     0        0       15           0   \n",
            "\n",
            "             rich  rule  set  step  sustain  understand  \n",
            "finance         0    21    1     1        0           1  \n",
            "education       2     0    0     1        0           0  \n",
            "environment     0     0    0     0       14           0  \n",
            "\n",
            "[3 rows x 21 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Get** **IDF** **for** **`each`** **word** idf=log((1+n)/(1+nt))+1"
      ],
      "metadata": {
        "id": "fLGOLwBgQWpq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the document frequency (number of documents containing each word)\n",
        "document_frequency = np.sum(term_counts > 0, axis=0)\n",
        "\n",
        "# Calculate IDF for each word\n",
        "total_documents = len(corpus )\n",
        "idf = {}\n",
        "for word, word_idx in zip(feature_names, range(len(feature_names))):\n",
        "    word_df = document_frequency[0, word_idx]\n",
        "    idf[word] = np.log((total_documents + 1) / (word_df + 1)) + 1   #n=total_documents ,nt=word_df\n",
        "\n",
        "# Print IDF for each word\n",
        "print(\"Inverse Document Frequency (IDF) for each word:\")\n",
        "for word, idf_value in idf.items():\n",
        "    print(\"{}: {}\".format(word, idf_value))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-IEkgssFuQ3",
        "outputId": "155fe6a8-d535-4f89-f3a5-ba8830e355d3"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inverse Document Frequency (IDF) for each word:\n",
            "american: 1.6931471805599454\n",
            "base: 1.2876820724517808\n",
            "child: 1.6931471805599454\n",
            "complex: 1.6931471805599454\n",
            "environ: 1.6931471805599454\n",
            "environment: 1.6931471805599454\n",
            "epa: 1.6931471805599454\n",
            "largest: 1.6931471805599454\n",
            "modern: 1.6931471805599454\n",
            "nation: 1.6931471805599454\n",
            "new: 1.6931471805599454\n",
            "poor: 1.6931471805599454\n",
            "process: 1.6931471805599454\n",
            "protect: 1.6931471805599454\n",
            "revolution: 1.6931471805599454\n",
            "rich: 1.6931471805599454\n",
            "rule: 1.6931471805599454\n",
            "set: 1.6931471805599454\n",
            "step: 1.2876820724517808\n",
            "sustain: 1.6931471805599454\n",
            "understand: 1.6931471805599454\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multiply**  **TF** * **IDF**"
      ],
      "metadata": {
        "id": "E9zYxnEfRAQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame to store TF * IDF values\n",
        "tf_idf_df = pd.DataFrame(index=processed_documents.keys(), columns=feature_names)\n",
        "\n",
        "# Multiply TF * IDF and store the result in the DataFrame\n",
        "for word in feature_names:\n",
        "    tf_values = tf_df[word].values  # TF values for the word\n",
        "    idf_value = idf[word]  # IDF value for the word\n",
        "\n",
        "    tf_idf_values = tf_values * idf_value  # Multiply TF with IDF\n",
        "    tf_idf_df[word] = tf_idf_values\n",
        "\n",
        "# Print the DataFrame\n",
        "print(\"TF * IDF for each word in all documents:\")\n",
        "print(tf_idf_df)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_gW5T40XxBK",
        "outputId": "eb36a30b-8e9a-4cad-e7c1-43ebf488f43d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF * IDF for each word in all documents:\n",
            "             american       base      child   complex    environ  environment  \\\n",
            "finance      0.000000  14.164503   0.000000  5.079442   0.000000     0.000000   \n",
            "education    3.386294   1.287682  20.317766  0.000000   0.000000     0.000000   \n",
            "environment  0.000000   0.000000   0.000000  0.000000  25.397208     1.693147   \n",
            "\n",
            "                   epa   largest    modern    nation  ...       poor  \\\n",
            "finance       0.000000  0.000000  1.693147  0.000000  ...   0.000000   \n",
            "education     0.000000  0.000000  0.000000  0.000000  ...  15.238325   \n",
            "environment  22.010913  1.693147  0.000000  1.693147  ...   0.000000   \n",
            "\n",
            "              process    protect  revolution      rich       rule       set  \\\n",
            "finance      0.000000   0.000000    0.000000  0.000000  35.556091  1.693147   \n",
            "education    1.693147   0.000000    1.693147  3.386294   0.000000  0.000000   \n",
            "environment  0.000000  25.397208    0.000000  0.000000   0.000000  0.000000   \n",
            "\n",
            "                 step    sustain  understand  \n",
            "finance      1.287682   0.000000    1.693147  \n",
            "education    1.287682   0.000000    0.000000  \n",
            "environment  0.000000  23.704061    0.000000  \n",
            "\n",
            "[3 rows x 21 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Get** **Normalized** **TFIDF** =tf-idf(t,d,D)/sqrt(sum of squares of all tf-idf(t,d))"
      ],
      "metadata": {
        "id": "OqSIwO7DRNX6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the sum of squares of all TF-IDF scores for each document\n",
        "sum_of_squares = np.sqrt(np.sum(np.square(tf_idf_df), axis=1))\n",
        "\n",
        "# Normalize TF-IDF scores for each document\n",
        "normalized_tf_idf_df= tf_idf_df.div(sum_of_squares, axis=0)\n",
        "\n",
        "# Print the normalized TF-IDF table\n",
        "print(\"Normalized TF-IDF (Term Frequency-Inverse Document Frequency) for each word in all documents:\")\n",
        "print(normalized_tf_idf_df)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lNE6Il-Mv6f",
        "outputId": "88879392-a715-412b-a10a-420d58290db7"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalized TF-IDF (Term Frequency-Inverse Document Frequency) for each word in all documents:\n",
            "             american      base     child  complex   environ  environment  \\\n",
            "finance      0.000000  0.365613  0.000000  0.13111  0.000000     0.000000   \n",
            "education    0.129871  0.049385  0.779226  0.00000  0.000000     0.000000   \n",
            "environment  0.000000  0.000000  0.000000  0.00000  0.524463     0.034964   \n",
            "\n",
            "                  epa   largest    modern    nation  ...     poor   process  \\\n",
            "finance      0.000000  0.000000  0.043703  0.000000  ...  0.00000  0.000000   \n",
            "education    0.000000  0.000000  0.000000  0.000000  ...  0.58442  0.064936   \n",
            "environment  0.454534  0.034964  0.000000  0.034964  ...  0.00000  0.000000   \n",
            "\n",
            "              protect  revolution      rich      rule       set      step  \\\n",
            "finance      0.000000    0.000000  0.000000  0.917771  0.043703  0.033238   \n",
            "education    0.000000    0.064936  0.129871  0.000000  0.000000  0.049385   \n",
            "environment  0.524463    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "\n",
            "              sustain  understand  \n",
            "finance      0.000000    0.043703  \n",
            "education    0.000000    0.000000  \n",
            "environment  0.489499    0.000000  \n",
            "\n",
            "[3 rows x 21 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Normalized TF-IDF (Term Frequency-Inverse Document Frequency) for each word in all documents:\")\n",
        "for index, row in normalized_tf_idf_df.iterrows():\n",
        "    values_line = '     '.join([f\"{value:.8f}\" for value in row.values])\n",
        "    print( \"[  \"+values_line+\" ]\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CXyDO_2RG-N",
        "outputId": "62a77f36-e09c-4337-d59c-a4d795718a5f"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalized TF-IDF (Term Frequency-Inverse Document Frequency) for each word in all documents:\n",
            "[  0.00000000     0.36561283     0.00000000     0.13111007     0.00000000     0.00000000     0.00000000     0.00000000     0.04370336     0.00000000     0.00000000     0.00000000     0.00000000     0.00000000     0.00000000     0.00000000     0.91777052     0.04370336     0.03323753     0.00000000     0.04370336 ]\n",
            "[  0.12987101     0.04938512     0.77922604     0.00000000     0.00000000     0.00000000     0.00000000     0.00000000     0.00000000     0.00000000     0.06493550     0.58441953     0.06493550     0.00000000     0.06493550     0.12987101     0.00000000     0.00000000     0.04938512     0.00000000     0.00000000 ]\n",
            "[  0.00000000     0.00000000     0.00000000     0.00000000     0.52446270     0.03496418     0.45453434     0.03496418     0.00000000     0.03496418     0.00000000     0.00000000     0.00000000     0.52446270     0.00000000     0.00000000     0.00000000     0.00000000     0.00000000     0.48949852     0.00000000 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TF IDF built in**"
      ],
      "metadata": {
        "id": "jV2eBjsgSZ6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# TF-IDF calculation using scikit-learn\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(preprocessed_documents)\n",
        "\n",
        "print(\"TF-IDF Matrix (scikit-learn):\\n\", tfidf_matrix.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBaKus2RReRq",
        "outputId": "3c0eb800-9cec-41bb-9cbb-2e0e2c9154c0"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Matrix (scikit-learn):\n",
            " [[0.         0.36561283 0.         0.13111007 0.         0.\n",
            "  0.         0.         0.04370336 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.91777052 0.04370336\n",
            "  0.03323753 0.         0.04370336]\n",
            " [0.12987101 0.04938512 0.77922604 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.0649355  0.58441953\n",
            "  0.0649355  0.         0.0649355  0.12987101 0.         0.\n",
            "  0.04938512 0.         0.        ]\n",
            " [0.         0.         0.         0.         0.5244627  0.03496418\n",
            "  0.45453434 0.03496418 0.         0.03496418 0.         0.\n",
            "  0.         0.5244627  0.         0.         0.         0.\n",
            "  0.         0.48949852 0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Different way From scratch using normalize from sklearn**"
      ],
      "metadata": {
        "id": "mJN4Y0zqTAua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import normalize\n",
        "import numpy as np\n",
        "\n",
        "total_docs = len(preprocessed_documents)\n",
        "word_idf = {}\n",
        "for word in count_vectorizer.vocabulary_:\n",
        "    word_docs_count = sum(1 for doc in preprocessed_documents if word in doc)\n",
        "    word_idf[word] = total_docs / (1 + word_docs_count)\n",
        "\n",
        "# Convert term_freq_matrix to float64\n",
        "term_freq_matrix = term_freq_matrix.astype(np.float64)\n",
        "\n",
        "# Multiply TF * IDF\n",
        "tfidf_matrix_scratch = term_freq_matrix.copy()\n",
        "for word, idx in count_vectorizer.vocabulary_.items():\n",
        "    tfidf_matrix_scratch[:, idx] *= word_idf[word]\n",
        "\n",
        "# Normalize TF-IDF\n",
        "tfidf_matrix_scratch = normalize(tfidf_matrix_scratch)\n",
        "\n",
        "print(\"\\nTF-IDF Matrix (from scratch):\\n\", tfidf_matrix_scratch.toarray())\n",
        "\n"
      ],
      "metadata": {
        "id": "tYt78_ZMCma6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9284eb92-762e-4d85-8cf9-76c61798b00f"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "TF-IDF Matrix (from scratch):\n",
            " [[0.         0.32561342 0.         0.13320549 0.         0.\n",
            "  0.         0.         0.04440183 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.93243842 0.04440183\n",
            "  0.02960122 0.         0.04440183]\n",
            " [0.12994442 0.04331481 0.77966655 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.06497221 0.58474991\n",
            "  0.06497221 0.         0.06497221 0.12994442 0.         0.\n",
            "  0.04331481 0.         0.        ]\n",
            " [0.         0.         0.         0.         0.5244627  0.03496418\n",
            "  0.45453434 0.03496418 0.         0.03496418 0.         0.\n",
            "  0.         0.5244627  0.         0.         0.         0.\n",
            "  0.         0.48949852 0.        ]]\n"
          ]
        }
      ]
    }
  ]
}